{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19cd58f5ac5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, json, PIL, shutil, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %pip install kaggle_datasets\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "# import KaggleDatasets \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import Model, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print(f'Running on TPU {tpu.master()}')\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KaggleDatasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antho\\OneDrive\\Desktop\\Monet-convert\\monet.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antho/OneDrive/Desktop/Monet-convert/monet.ipynb#ch0000003?line=0'>1</a>\u001b[0m GCS_PATH \u001b[39m=\u001b[39m KaggleDatasets()\u001b[39m.\u001b[39mget_gcs_path(\u001b[39m'\u001b[39m\u001b[39mmonet-gan-getting-started\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antho/OneDrive/Desktop/Monet-convert/monet.ipynb#ch0000003?line=2'>3</a>\u001b[0m MONET_FILENAMES \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mglob(\u001b[39mstr\u001b[39m(GCS_PATH \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/monet_tfrec/*.tfrec\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antho/OneDrive/Desktop/Monet-convert/monet.ipynb#ch0000003?line=3'>4</a>\u001b[0m PHOTO_FILENAMES \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mglob(\u001b[39mstr\u001b[39m(GCS_PATH \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/photo_tfrec/*.tfrec\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KaggleDatasets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('monet-gan-getting-started')\n",
    "\n",
    "MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n",
    "PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "n_monet_samples = count_data_items(MONET_FILENAMES)\n",
    "n_photo_samples = count_data_items(PHOTO_FILENAMES)\n",
    "\n",
    "print(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\n",
    "print(f'Monet image files: {n_monet_samples}')\n",
    "print(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')\n",
    "print(f'Photo image files: {n_photo_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'target':     tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_gan_dataset(monet_files, photo_files, augment=None, repeat=True, shuffle=True, batch_size=1):\n",
    "\n",
    "    monet_ds = load_dataset(monet_files)\n",
    "    photo_ds = load_dataset(photo_files)\n",
    "\n",
    "    if repeat:\n",
    "        monet_ds = monet_ds.repeat()\n",
    "        photo_ds = photo_ds.repeat()\n",
    "    if shuffle:\n",
    "        monet_ds = monet_ds.shuffle(2048)\n",
    "        photo_ds = photo_ds.shuffle(2048)\n",
    "        \n",
    "    monet_ds = monet_ds.batch(batch_size, drop_remainder=True)\n",
    "    photo_ds = photo_ds.batch(batch_size, drop_remainder=True)\n",
    "    monet_ds = monet_ds.cache()\n",
    "    photo_ds = photo_ds.cache()\n",
    "    monet_ds = monet_ds.prefetch(AUTO)\n",
    "    photo_ds = photo_ds.prefetch(AUTO)\n",
    "    \n",
    "    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n",
    "    \n",
    "    return gan_ds\n",
    "\n",
    "def display_samples(ds, row, col):\n",
    "    ds_iter = iter(ds)\n",
    "    plt.figure(figsize=(15, int(15*row/col)))\n",
    "    for j in range(row*col):\n",
    "        example_sample = next(ds_iter)\n",
    "        plt.subplot(row,col,j+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
    "    plt.show()\n",
    "        \n",
    "def display_generated_samples(ds, model, n_samples):\n",
    "    ds_iter = iter(ds)\n",
    "    for n_sample in range(n_samples):\n",
    "        example_sample = next(ds_iter)\n",
    "        generated_sample = model.predict(example_sample)\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        plt.title(\"input image\")\n",
    "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.title(\"Generated image\")\n",
    "        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "def predict_and_save(input_ds, generator_model, output_path):\n",
    "    i = 1\n",
    "    for img in input_ds:\n",
    "        prediction = generator_model(img, training=False)[0].numpy() # make predition\n",
    "        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   # re-scale\n",
    "        im = PIL.Image.fromarray(prediction)\n",
    "        im.save(f'{output_path}{str(i)}.jpg')\n",
    "        i += 1\n",
    "                \n",
    "\n",
    "# Model functions\n",
    "def downsample(filters, size, apply_instancenorm=True, strides=2):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(L.Conv2D(filters, size, strides=strides, padding='same',\n",
    "                        kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_instancenorm:\n",
    "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    result.add(L.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False, strides=2):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(L.Conv2DTranspose(filters, size, strides=strides, padding='same',\n",
    "                                 kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(L.Dropout(0.5))\n",
    "\n",
    "    result.add(L.ReLU())\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7306c8970b0170d37f7b03abe0a3ca3c637f9604f9b9c56852298292bebaba93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
